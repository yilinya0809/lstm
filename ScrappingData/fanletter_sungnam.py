# -*- coding: utf-8 -*-
"""Crawling_fanletter_sungnam_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JuwkZQOChAIAcJq3ul07BjU4zsUnhaOT
"""

from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

import time

### Crawling Setup
browser = webdriver.Chrome(executable_path='./chromedriver')
browser.implicitly_wait(3)

## 팬레터 성남 후기(웹)

url='https://tickets.interpark.com/goods/21000818'
browser.get(url)

## 관람후기 클릭 시뮬레이션

browser.find_element_by_xpath('//*[@id="productMainBody"]/nav/div/div/ul/li[3]/a').click()

import pandas as pd


reviews = []
score = []


for page in range(1,6):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a[2]').click()

df=pd.DataFrame({'Reviews': reviews, 'Score':score})
df.to_csv('fanletter_sungnam.csv', encoding = "utf-8-sig")

