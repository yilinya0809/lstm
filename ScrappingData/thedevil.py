# -*- coding: utf-8 -*-
"""Crawling_Thedevil.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RpnE5hxAWPQwqdtZjxPfIoMxRTvHgHW5
"""

from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

import time

### Crawling Setup
browser = webdriver.Chrome(executable_path='./chromedriver')
browser.implicitly_wait(3)

##사의찬미 2015

url='https://tickets.interpark.com/goods/15004097'
browser.get(url)

## 관람후기 클릭 시뮬레이션

browser.find_element_by_xpath('//*[@id="productMainBody"]/nav/div/div/ul/li[3]/a').click()

import pandas as pd


reviews = []
score = []


for page in range(1,37):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a[2]').click()

for page in range(5,37):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a[2]').click()

for page in range(34,37):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a[2]').click()

df=pd.DataFrame({'Reviews': reviews, 'Score':score})
df.to_csv('Gloomyday2015.csv', encoding = "utf-8-sig")

##글루미데이 2014

url='https://tickets.interpark.com/goods/14000303'
browser.get(url)

## 관람후기 클릭 시뮬레이션

browser.find_element_by_xpath('//*[@id="productMainBody"]/nav/div/div/ul/li[3]/a').click()

import pandas as pd


reviews = []
score = []


for page in range(1,18):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a[2]').click()

df2=pd.DataFrame({'Reviews': reviews, 'Score':score})
df2.to_csv('Gloomyday2014.csv', encoding = "utf-8-sig")

##글루미데이 2013

url='https://tickets.interpark.com/goods/13004809'
browser.get(url)

## 관람후기 클릭 시뮬레이션

browser.find_element_by_xpath('//*[@id="productMainBody"]/nav/div/div/ul/li[3]/a').click()

import pandas as pd


reviews = []
score = []


for page in range(1,3):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/a[2]').click()

df3=pd.DataFrame({'Reviews': reviews, 'Score':score})
df3.to_csv('Gloomyday2013.csv', encoding = "utf-8-sig")

##그림자를 판 사나이

url='https://tickets.interpark.com/goods/19013468'
browser.get(url)

## 관람후기 클릭 시뮬레이션

browser.find_element_by_xpath('//*[@id="productMainBody"]/nav/div/div/ul/li[3]/a').click()

import pandas as pd


reviews = []
score = []


for page in range(1,17):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a[2]').click()

df4=pd.DataFrame({'Reviews': reviews, 'Score':score})
df4.to_csv('Shadow.csv', encoding = "utf-8-sig")

##더 데빌 2014

url='https://tickets.interpark.com/goods/14007618'
browser.get(url)

## 관람후기 클릭 시뮬레이션

browser.find_element_by_xpath('//*[@id="productMainBody"]/nav/div/div/ul/li[3]/a').click()

import pandas as pd


reviews = []
score = []


for page in range(1,13):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a[2]').click()

for page in range(11,13):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a[2]').click()

df5=pd.DataFrame({'Reviews': reviews, 'Score':score})
df5.to_csv('TheDevil2014.csv', encoding = "utf-8-sig")

##더 데빌 2017

url='https://tickets.interpark.com/goods/16016338'
browser.get(url)

## 관람후기 클릭 시뮬레이션

browser.find_element_by_xpath('//*[@id="productMainBody"]/nav/div/div/ul/li[3]/a').click()

import pandas as pd


reviews = []
score = []


for page in range(1,18):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/a[2]').click()

df6=pd.DataFrame({'Reviews': reviews, 'Score':score})
df6.to_csv('TheDevil2017.csv', encoding = "utf-8-sig")

##더 데빌 2018

url='https://tickets.interpark.com/goods/18012426'
browser.get(url)

## 관람후기 클릭 시뮬레이션

browser.find_element_by_xpath('//*[@id="productMainBody"]/nav/div/div/ul/li[4]/a').click()

import pandas as pd


reviews = []
score = []


for page in range(1,18):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/a[2]').click()

for page in range(18,31):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/a[2]').click()

df7=pd.DataFrame({'Reviews': reviews, 'Score':score})
df7.to_csv('TheDevil2018.csv', encoding = "utf-8-sig")

