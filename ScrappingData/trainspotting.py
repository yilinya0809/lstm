# -*- coding: utf-8 -*-
"""Crawling_trainspotting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sirQNCvtlL5d7ZTAYT9wazhHqnx5aErM
"""

from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

import time

### Crawling Setup
browser = webdriver.Chrome(executable_path='./chromedriver')
browser.implicitly_wait(3)

##트레인스포팅

url='https://tickets.interpark.com/goods/18001402'
browser.get(url)

## 관람후기 클릭 시뮬레이션

browser.find_element_by_xpath('//*[@id="productMainBody"]/nav/div/div/ul/li[4]/a').click()

import pandas as pd


reviews = []
score = []


for page in range(1,29):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/a[2]').click()

df=pd.DataFrame({'Reviews': reviews, 'Score':score})
df.to_csv('Trainspotting.csv', encoding = "utf-8-sig")

##슬루스

url='https://tickets.interpark.com/goods/17006089'
browser.get(url)

## 관람후기 클릭 시뮬레이션

browser.find_element_by_xpath('//*[@id="productMainBody"]/nav/div/div/ul/li[3]/a').click()

import pandas as pd


reviews = []
score = []


for page in range(1,29):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[4]/div[2]/a[2]').click()

df2=pd.DataFrame({'Reviews': reviews, 'Score':score})
df2.to_csv('Sleuth.csv', encoding = "utf-8-sig")

##인터뷰 2017

url='https://tickets.interpark.com/goods/17006304'
browser.get(url)

## 관람후기 클릭 시뮬레이션

browser.find_element_by_xpath('//*[@id="productMainBody"]/nav/div/div/ul/li[4]/a').click()

import pandas as pd


reviews = []
score = []


for page in range(1,17):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a[2]').click()

df3=pd.DataFrame({'Reviews': reviews, 'Score':score})
df3.to_csv('Interview2017.csv', encoding = "utf-8-sig")

##인터뷰 2018

url='https://tickets.interpark.com/goods/18007802'
browser.get(url)

## 관람후기 클릭 시뮬레이션

browser.find_element_by_xpath('//*[@id="productMainBody"]/nav/div/div/ul/li[4]/a').click()

import pandas as pd


reviews = []
score = []


for page in range(1,17):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a[2]').click()

for page in range(17,29):
    if page == 1:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                         score.append(1)
                    else:
                         score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a').click()
    
    else:
        for i in range(1,11):
            if i == 1:
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
        
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())        
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
                
                
            else:
                browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/ol/li['+ str(i) +']/a').click()
                time.sleep(0.1)
    
                html = browser.page_source
                soup = BeautifulSoup(html, 'html.parser')
    
                for j in range(15):
                    reviews.append(soup.find_all("strong", {"class": "bbsTitleText"})[j].get_text() + ' ' + soup.find_all("p", {"class": "bbsText"})[j].get_text())
                    if soup.select('div.prdStarScore > span.blind')[j+2].get_text() == "평점: 강력 추천":
                        score.append(1)
                    else:
                        score.append(0)
        browser.find_element_by_xpath('//*[@id="prdReview"]/div/div[3]/div[2]/a[2]').click()

df4=pd.DataFrame({'Reviews': reviews, 'Score':score})
df4.to_csv('Interview2018.csv', encoding = "utf-8-sig")

